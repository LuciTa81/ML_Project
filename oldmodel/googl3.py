
import os, random, numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms, models

# ëœë¤ ì‹œë“œ ê³ ì •
SEED = 42
random.seed(SEED); np.random.seed(SEED)
torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False

# ë°ì´í„° ê²½ë¡œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
DATA_DIR = '/workspace/Images'
IMG_SIZE = 299; BATCH_SIZE = 8; LR = 0.0001; EPOCHS = 10; NUM_CLASSES = 120

# ë°ì´í„°ì…‹ ë° ë¡œë”
transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

dataset = datasets.ImageFolder(DATA_DIR, transform=transform)
total = len(dataset)
train_cnt = int(total*0.7); valid_cnt = int(total*0.1); test_cnt  = total - train_cnt - valid_cnt
train_set, valid_set, test_set = random_split(dataset, [train_cnt, valid_cnt, test_cnt],
                                            generator=torch.Generator().manual_seed(SEED))

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)

# ëª¨ë¸ ì •ì˜ (pretrained=True ëŒ€ì‹  ìµœì‹  ë°©ì‹ì¸ weights ì‚¬ìš© ê¶Œì¥)
model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)
model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)

# í•™ìŠµ ë£¨í”„
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

print("GoogLeNet í•™ìŠµ ì‹œì‘...")

for epoch in range(EPOCHS):
    model.train() # ğŸ‘ˆ Train ëª¨ë“œ í™œì„±í™”
    total_loss, total_correct = 0,0
    for x,y in train_loader:
        x,y = x.to(device), y.to(device)
        optimizer.zero_grad()

        pred = model(x) # ğŸ‘ˆ predëŠ” Tensor ê°ì²´ë¡œ ë°˜í™˜ë¨ (ì˜¤ë¥˜ ë©”ì‹œì§€ ê¸°ì¤€)

        # âœ… [ìˆ˜ì •] ê°ì²´ê°€ ì•„ë‹Œ Tensorì´ë¯€ë¡œ, .logits ì—†ì´ predë¥¼ ì§ì ‘ ì‚¬ìš©
        loss = criterion(pred, y) 

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        # âœ… [ìˆ˜ì •] main_output ëŒ€ì‹  predë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ ê³„ì‚°
        total_correct += (pred.argmax(1)==y).sum().item()

    train_acc = total_correct/len(train_set)
    train_loss_avg = total_loss / len(train_loader)

    model.eval() # ğŸ‘ˆ Eval ëª¨ë“œ í™œì„±í™”
    vloss, vcorrect = 0,0
    with torch.no_grad():
        for x,y in valid_loader:
            x,y = x.to(device), y.to(device)

            pred = model(x) # ğŸ‘ˆ Eval ëª¨ë“œëŠ” ì›ë˜ Tensorë¥¼ ë°˜í™˜ (ìˆ˜ì • ë¶ˆí•„ìš”)

            loss = criterion(pred,y) # ğŸ‘ˆ (ìˆ˜ì • ë¶ˆí•„ìš”)
            vloss += loss.item()
            vcorrect += (pred.argmax(1)==y).sum().item() # ğŸ‘ˆ (ìˆ˜ì • ë¶ˆí•„ìš”)

    valid_acc = vcorrect/len(valid_set)
    valid_loss_avg = vloss / len(valid_loader)

    # âœ… Loss ê°’ í¬í•¨í•˜ì—¬ ì¶œë ¥
    print(f"[{epoch+1}/{EPOCHS}] Train Loss:{train_loss_avg:.4f}, Train Acc:{train_acc:.4f} | Valid Loss:{valid_loss_avg:.4f}, Valid Acc:{valid_acc:.4f}")

print("âœ… GoogLeNet í•™ìŠµ ì™„ë£Œ!")
